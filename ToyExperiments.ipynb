{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6lbnWXg1j5e"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ehfO-SKvewiJ",
    "outputId": "2554898e-5b62-4997-caf0-d524a5be97b4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import autograd\n",
    "import altair as alt\n",
    "alt.data_transformers.enable('default', max_rows=None)\n",
    "import pandas as pd\n",
    "import math\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJJsnk6Z1quI"
   },
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7bKliL2SezLL"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DfMAF2m1u1H"
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6hrpQy2Te3g5"
   },
   "outputs": [],
   "source": [
    "def get_density_chart(P, d=7.0, step=0.1, dist_type=None):\n",
    "    \"\"\"\n",
    "    Given a probability distribution, return a density chart (Heatmap)\n",
    "    Inputs:\n",
    "        P: Probability distribution\n",
    "        d: A value used to bound the meshgrid\n",
    "        step: A value used in the arange method to create the meshgrid\n",
    "    Outputs:\n",
    "        chart: Altair object corresponding to a density plot\n",
    "    \"\"\"\n",
    "    xv, yv = torch.meshgrid([torch.arange(-d, d, step), torch.arange(-d, d, step)])\n",
    "    pos_xy = torch.cat((xv.unsqueeze(-1), yv.unsqueeze(-1)), dim=-1)\n",
    "    p_xy = P.log_prob(pos_xy.to(device)).exp().unsqueeze(-1).cpu()\n",
    "\n",
    "\n",
    "    df = torch.cat([pos_xy, p_xy], dim=-1).numpy()\n",
    "    df = pd.DataFrame({\n",
    "        'x': df[:, :, 0].ravel(),\n",
    "        'y': df[:, :, 1].ravel(),\n",
    "        'p': df[:, :, 2].ravel(),})\n",
    "\n",
    "    chart = alt.Chart(df).mark_point().encode(\n",
    "\n",
    "        x=alt.X('x:Q', axis=alt.Axis(title='', labelFontSize=20, tickSize=12)),\n",
    "        y=alt.Y('y:Q', axis=alt.Axis(title='', labelFontSize=20, tickSize=12)),\n",
    "        color=alt.Color('p:Q', scale=alt.Scale(scheme='lightorange')),\n",
    "        tooltip=['x','y','p']).properties(\n",
    "        width=220,\n",
    "        height=190\n",
    "    )\n",
    "\n",
    "\n",
    "    return chart\n",
    "\n",
    "\n",
    "\n",
    "def get_particles_chart(X, X_svgd=None):\n",
    "    \"\"\"\n",
    "    Given a set of points, return a scatter plot\n",
    "    Inputs:\n",
    "        X: Final positions of the particles after applying svgd\n",
    "        X_svgd: Intermidiate position of the particles while applying svgd. If None do not add them to the plot\n",
    "    Outputs:\n",
    "        chart: Altair object corresponding to a scatter plot\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'x': X[:, 0],\n",
    "        'y': X[:, 1],})\n",
    "\n",
    "    chart = alt.Chart(df).mark_circle(color='black').encode(x='x:Q',y='y:Q')\n",
    "\n",
    "    if X_svgd is not None:\n",
    "        for i in range(np.shape(X_svgd)[1]):\n",
    "            df_trajectory = pd.DataFrame({'x': X_svgd[:,i,0],'y': X_svgd[:,i,1],})\n",
    "            chart += alt.Chart(df_trajectory).mark_line().mark_circle(color='green').encode(x='x:Q',y='y:Q')\n",
    "\n",
    "    return chart\n",
    "\n",
    "\n",
    "def filter(results, constraints):\n",
    "    \"\"\"\n",
    "    Given a dictionary of experiments results, search it given a list of constraints\n",
    "    Inputs:\n",
    "        results: A dictionary of experimental results\n",
    "        constraints: A list of configuration constraints\n",
    "    Outputs:\n",
    "        Elements based on the input configurations of experiments \n",
    "    \"\"\"\n",
    "    configs = []\n",
    "    for i in range(len(results['configs'])):\n",
    "        check_bool = True\n",
    "        for j in range(len(results['configs'][i])):\n",
    "            if constraints[j] == '*':\n",
    "                continue\n",
    "            if results['configs'][i][j] != constraints[j]:\n",
    "                check_bool = False\n",
    "                break\n",
    "        if check_bool:\n",
    "            configs.append(i)\n",
    "        \n",
    "    return np.array(results['sampler_entr_svgd'])[configs].tolist(), np.array(results['gt_entr'])[configs].tolist(), np.array(results['charts_svgd'])[configs].tolist(), np.array(results['init_chart'])[configs].tolist()\n",
    "\n",
    "def figure_4c(results, mu, sigma, figure_name='figure_4c'):\n",
    "    \"\"\"\n",
    "    Create figure 4c in the paper\n",
    "    Inputs:\n",
    "        results: Dictionary containing all the saved data\n",
    "        mu: Mean of the initial distribution\n",
    "        sigma: Standard deviation of the initial distribution\n",
    "        figure_name: Figure name\n",
    "    \"\"\"\n",
    "\n",
    "    x_values = ['0.1', '1', '3', '5', '7', '10', '100']\n",
    "    entropies, gt_entr, charts, init_charts = filter(results, ['*', '*', '*', '*', 200, 200, '*' , mu, sigma])\n",
    "    dis_200s_200p = entropies\n",
    "    entropies, gt_entr, charts, init_charts = filter(results, ['*', '*', '*', '*', 100, 100, '*' , mu, sigma])\n",
    "    dis_100s_100p = entropies\n",
    "    entropies, gt_entr, charts, init_charts = filter(results, ['*', '*', '*', '*', 10, 20, '*' , mu, sigma])\n",
    "    dis_20s_10p = entropies\n",
    "    entropies, gt_entr, charts, init_charts = filter(results, ['*', '*', '*', '*', 20, 20, '*' , mu, sigma])\n",
    "    dis_20s_20p = entropies\n",
    "\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    fig = plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(x_values, dis_200s_200p, marker=\"o\", label=\"200\", s=60 )\n",
    "    plt.scatter(x_values, dis_100s_100p, marker=\"s\", label=\"100\", s=60 )\n",
    "    plt.scatter(x_values, dis_20s_20p, marker=\"P\", label=\"20\", s=60 )\n",
    "    plt.scatter(x_values, dis_20s_10p, marker=\"*\", label=\"10\", s=60 )\n",
    "    plt.axhline(y = np.mean(gt_entr), color = 'g', linestyle = 'dotted')\n",
    "    plt.legend(title=\"# particles\", loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.xlabel(\"Kernel variance\")\n",
    "    plt.ylabel(r\"$H(q^L)$\")\n",
    "    plt.ylim(0, 5)\n",
    "    plt.savefig('./' + figure_name + '.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def figure_4b(results, dims, expr_name, x_label, x_ticks, results_tmp=None, results_200=None, resutls_500=None):\n",
    "    \"\"\"\n",
    "    Create figure 4b from the paper\n",
    "    Inputs:\n",
    "        results: Dictionary containing all the results' data\n",
    "        dims: A list containing the dimensions we want to print\n",
    "        expr_name: Experiment name\n",
    "        x_label: X axis label\n",
    "        x_ticks: X axis ticks\n",
    "    \"\"\"\n",
    "\n",
    "    for dim in dims:\n",
    "        filtered_results = [filter(results[i][expr_name], [dim, '*', '*', '*', 10, 20, '*' , 0, 6]) for i in range(len(results))]\n",
    "        gmm1_20s_10p = [filtered_results[i][0] for i in range(len(filtered_results))]\n",
    "\n",
    "        if results_tmp == None:\n",
    "            filtered_results = [filter(results[i][expr_name], [dim, '*', '*', '*', 10, 10, '*' , 0, 6]) for i in range(len(results))]\n",
    "        else:\n",
    "            filtered_results = [filter(results_tmp[i][expr_name], [dim, '*', '*', '*', 10, 10, '*' , 0, 6]) for i in range(len(results_tmp))]\n",
    "        gmm1_10s_10p = [filtered_results[i][0] for i in range(len(filtered_results))]\n",
    "        if results_200 == None:\n",
    "            filtered_results = [filter(results[i][expr_name], [dim, '*', '*', '*', 200, 200, '*' , 0, 6]) for i in range(len(results))]\n",
    "        else:\n",
    "            filtered_results = [filter(results_200[i][expr_name], [dim, '*', '*', '*', 200, 200, '*' , 0, 6]) for i in range(len(results_200))]\n",
    "        gmm1_200s_200p = [filtered_results[i][0] for i in range(len(filtered_results))]\n",
    "\n",
    "        if results_500 == None:\n",
    "            filtered_results = [filter(results[i][expr_name], [dim, '*', '*', '*', 200, 500, '*' , 0, 6]) for i in range(len(results))]\n",
    "        else:\n",
    "            filtered_results = [filter(resutls_500[i][expr_name], [dim, '*', '*', '*', 200, 200, '*' , 0, 6]) for i in range(len(resutls_500))]\n",
    "        gmm1_500s_200p = [filtered_results[i][0] for i in range(len(filtered_results))]\n",
    "        \n",
    "        \n",
    "        raw_mean = [list(np.nanmean(gmm1_20s_10p, axis=0)), list(np.nanmean(gmm1_10s_10p, axis=0)), list(np.nanmean(gmm1_200s_200p, axis=0)), list(np.nanmean(gmm1_500s_200p, axis=0))]\n",
    "        raw_std = [list(np.nanstd(gmm1_20s_10p, axis=0)), list(np.nanstd(gmm1_10s_10p, axis=0)), list(np.nanstd(gmm1_200s_200p, axis=0)), list(np.nanstd(gmm1_500s_200p, axis=0))]\n",
    "        colors = ['r', 'g', 'b', 'k']\n",
    "        labels = ['#(p,s): (10,20) ', '#(p,s): (10,10) ', '#(p,s): (200,200) ', '#(p,s): (200,500) ']\n",
    "        plt.figure(figsize=(8, 6), dpi=150)\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel(\"Entropy\")\n",
    "        for i in range(len(raw_mean)):\n",
    "            plt.errorbar(np.array([0.0, 1.0, 2.0, 3.0]), raw_mean[i], yerr=raw_std[i], color=colors[i], marker='o', label=labels[i])\n",
    "        plt.xticks(np.array([0.0, 1.0, 2.0, 3.0]), np.array(x_ticks))\n",
    "        plt.xlim([-0.5, 3.5])\n",
    "        plt.title('dim_' + str(dim))\n",
    "        plt.legend()\n",
    "        plt.savefig('./' + expr_name + '_' + str(dim) + '.png', bbox_inches='tight')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5J73Y-XM10MP"
   },
   "source": [
    "# Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBF:\n",
    "    \"\"\"\n",
    "    Radial basis funtion kernel (https://en.wikipedia.org/wiki/Radial_basis_function)\n",
    "    Inputs:\n",
    "        sigma: Kernel standard deviation \n",
    "        num_particles: Number of particles\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma, num_particles):\n",
    "        self.sigma = sigma\n",
    "        self.num_particles = num_particles\n",
    "\n",
    "    def forward(self, input_1, input_2):\n",
    "        \"\"\"\n",
    "        Given two tensors of particles, return the matrix of distances between each particle\n",
    "        Inputs:\n",
    "            input_1: Particles coordinates\n",
    "            input_2: Particles coordinates\n",
    "        Outputs:\n",
    "            kappa: RBF matrix \n",
    "            diff: Signed distance \n",
    "            h: Kernel variance\n",
    "            kappa_grad: Derivative of rbf kernel\n",
    "            gamma: 1/2*sigma**2\n",
    "        \"\"\"\n",
    "        # Check if the inputs have the same size in the last two dimensions\n",
    "        assert input_2.size()[-2:] == input_1.size()[-2:]\n",
    "        \n",
    "        # Compute the difference between each particle\n",
    "        diff = input_1.unsqueeze(-2) - input_2.unsqueeze(-3)\n",
    "        # Square the difference and sum over the particle's dimensions\n",
    "        dist_sq = diff.pow(2).sum(-1)\n",
    "        dist_sq = dist_sq.unsqueeze(-1)\n",
    "        \n",
    "        if self.sigma == \"mean\":\n",
    "            # Estimate the kernel variance using the mean of all the particles distances\n",
    "            median_sq = torch.mean(dist_sq.detach().reshape(-1, self.num_particles*self.num_particles), dim=1)\n",
    "            median_sq = median_sq.unsqueeze(1).unsqueeze(1)\n",
    "            h = median_sq / (2 * np.log(self.num_particles + 1.))\n",
    "            sigma = torch.sqrt(h)\n",
    "        elif self.sigma == \"forth\":\n",
    "            # Estimate the kernel variance using the mean devided by two (one forth) of all the particles distances\n",
    "            median_sq = 0.5 * torch.mean(dist_sq.detach().reshape(-1, self.num_particles*self.num_particles), dim=1)\n",
    "            median_sq = median_sq.unsqueeze(1).unsqueeze(1)\n",
    "            h = median_sq / (2 * np.log(self.num_particles + 1.))\n",
    "            sigma = torch.sqrt(h)\n",
    "        elif self.sigma == \"median\":\n",
    "            # Estimate the kernel variance using the median of all the particles distances\n",
    "            median_sq = torch.median(dist_sq.detach().reshape(-1, self.num_particles*self.num_particles), dim=1)[0]\n",
    "            median_sq = median_sq.unsqueeze(1).unsqueeze(1)\n",
    "            h = median_sq / (2 * np.log(self.num_particles + 1.))\n",
    "            sigma = torch.sqrt(h)\n",
    "        else:\n",
    "            # Setting the kernel variance from the input \n",
    "            sigma = self.sigma\n",
    "            h = None\n",
    "        \n",
    "        gamma = 1.0 / (1e-8 + 2 * sigma**2) \n",
    "        \n",
    "        kappa = (-gamma * dist_sq).exp() \n",
    "        # Computing the gradient of the kernel over the particles\n",
    "        kappa_grad = -2. * (diff * gamma) * kappa\n",
    "        return kappa.squeeze(), diff, h, kappa_grad, gamma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5J73Y-XM10MP"
   },
   "source": [
    "# GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GMMDist(object):\n",
    "    \"\"\"\n",
    "    Gaussian Mixture Model\n",
    "    Inputs:\n",
    "        dim: Number of dimensions of the random variable\n",
    "        n_gmm: Number of mixtures\n",
    "        sigma: Mixutures' standard diviation\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, n_gmm, sigma):\n",
    "        def _compute_mu(i):\n",
    "            \"\"\"\n",
    "            Compute the mean of each mixture\n",
    "            Inputs:\n",
    "                i: Index of the mixture\n",
    "            Outputs:\n",
    "                The mean of each dimension of the random variable\n",
    "            \"\"\"\n",
    "            return 4 * torch.Tensor([[torch.tensor(i * math.pi / (n_gmm/2)).sin(),torch.tensor(i * math.pi / (n_gmm/2)).cos()] + [torch.rand((1,)) * 2 - 1 for _ in range(dim - 2)]])\n",
    "        \n",
    "        # Generate a list of equal probabilities between the mixtures\n",
    "        self.mix_probs = (1.0/n_gmm) * torch.ones(n_gmm).to(device)\n",
    "        # Compute the means\n",
    "        self.means = torch.cat([_compute_mu(i) for i in range(n_gmm)], dim=0).to(device)\n",
    "        # Compute the standard deviations\n",
    "        self.sigma = sigma\n",
    "        self.std = torch.stack([torch.ones(dim).to(device) * self.sigma for i in range(len(self.mix_probs))], dim=0)\n",
    "        \n",
    "        \n",
    "    def sample(self, n):\n",
    "        \"\"\"\n",
    "        Sample a tensor of particles\n",
    "        Inputs:\n",
    "            n: Number of particles\n",
    "        Outputs:\n",
    "            A tensor of particles that follows this distribution\n",
    "        \"\"\"\n",
    "        mix_idx = torch.multinomial(self.mix_probs, n[0], replacement=True)\n",
    "        means = self.means[mix_idx]\n",
    "        stds = self.std[mix_idx]\n",
    "        return torch.randn_like(means) * stds + means\n",
    "\n",
    "    def log_prob(self, samples):\n",
    "        \"\"\"\n",
    "        Given a tensor of particles, compute their log probability with respect to the current distribution \n",
    "        (the mixtures are independent)\n",
    "        Inputs:\n",
    "            samples: A tensor of particles\n",
    "        Outputs:\n",
    "            A tensor of log probabilites\n",
    "        \"\"\"\n",
    "        logps = []\n",
    "        for i in range(len(self.mix_probs)):\n",
    "            logps.append((-((samples.to(device) - self.means[i]) ** 2).sum(dim=-1) / (2 * self.sigma ** 2) - 0.5 * np.log(\n",
    "                2 * np.pi * self.sigma ** 2)) + self.mix_probs[i].log())\n",
    "        logp = torch.logsumexp(torch.stack(logps, dim=0), dim=0)\n",
    "        return logp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNVtrc_V15JL"
   },
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFRnfC_6e36w"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Optim():\n",
    "    \"\"\"\n",
    "    Optimizer Class \n",
    "    Inputs:\n",
    "        lr: Learning rate\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=None):\n",
    "        self.m_dx, self.v_dx = 0, 0\n",
    "        self.lr = lr\n",
    "    \n",
    "    def step(self,x, dx): \n",
    "        \"\"\"\n",
    "        Gradient Ascent\n",
    "        Inputs:\n",
    "            x: A tensor of particles\n",
    "            dx: The update vector\n",
    "        Outputs:\n",
    "            x: Updated particles\n",
    "        \"\"\"\n",
    "        dx = dx.view(x.size())\n",
    "        x = x + self.lr * dx \n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aKJMc3XC14PX"
   },
   "source": [
    "# Entropy Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5IgmzBKe39j"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Entropy():\n",
    "    \"\"\"\n",
    "    Entropy Class \n",
    "    Inputs:\n",
    "        P: GMM Object\n",
    "        K: RBF Kernel\n",
    "        optimizer: Gradient Ascent optimizer\n",
    "        num_particles: Number of particles\n",
    "        particles_dim: Particles' number of dimensions \n",
    "    \"\"\"\n",
    "    def __init__(self, P, K, optimizer, num_particles, particles_dim):\n",
    "        self.P = P\n",
    "        self.optim = optimizer\n",
    "        self.num_particles = num_particles\n",
    "        self.particles_dim = particles_dim\n",
    "        self.K = K\n",
    "\n",
    "        # Mean of Langevin dynamics initial distribution\n",
    "        mu_ld_noise = torch.zeros((self.particles_dim,)) \n",
    "        # Standard deviation of Langevin dynamics initial distribution\n",
    "        sigma_ld_noise = torch.eye(self.particles_dim) * 0.1\n",
    "        # Langevin dynamics initial distribution\n",
    "        self.init_dist_ld = torch.distributions.MultivariateNormal(mu_ld_noise,covariance_matrix=sigma_ld_noise)\n",
    "        # Identity matrix used in the computation of the entropy (line 1)\n",
    "        self.identity_mat = torch.eye(self.particles_dim).to(device)\n",
    "        # Identity matrix used in the computation of the entropy (line 3)\n",
    "        self.identity_mat2 = torch.eye(self.num_particles).to(device)\n",
    "        # Entropy Variables, Line 1,2 and 3\n",
    "        self.logp_line1 = 0\n",
    "        self.logp_line2 = 0\n",
    "        self.logp_line3 = 0\n",
    "\n",
    "        # HMC initilization\n",
    "\n",
    "        self.X_2 = None\n",
    "        self.V_2 = None\n",
    "    \n",
    "    def SVGD(self,X):\n",
    "        \"\"\"\n",
    "        Compute the Stein Variational Gradient given a set of particles\n",
    "        Inputs:\n",
    "            X: A tensor of particles\n",
    "        Outputs:\n",
    "            phi: SVGD update rule\n",
    "            phi_log_prob: SVGD update without considering the distance of the particle to itself \n",
    "            (used to calculate the log prob)\n",
    "        \"\"\"\n",
    "        # Make X requires grad\n",
    "        X = X.requires_grad_(True)\n",
    "        # Compute the log probability  of the particles using the target distribution\n",
    "        log_prob = self.P.log_prob(X)\n",
    "\n",
    "        # Compute the gradient of the log probability over the particles\n",
    "        self.score_func = autograd.grad(log_prob.sum(), X, retain_graph=True, create_graph=True)[0].reshape(self.num_particles, self.particles_dim)\n",
    "        # Compute the kernel matrices         \n",
    "        self.K_XX, self.K_diff, self.K_h, self.K_grad, self.K_gamma = self.K.forward(X, X)  \n",
    "\n",
    "        # Compute the first term of the  SVGD update \n",
    "        self.phi_term1 = self.K_XX.matmul(self.score_func) / self.num_particles\n",
    "        # Compute the second term of the  SVGD update \n",
    "        self.phi_term2 = self.K_grad.sum(0) / self.num_particles\n",
    "        # Compute the SVGD update\n",
    "        phi = self.phi_term1 + self.phi_term2\n",
    "        \n",
    "        # Compute SVGD update without considering the distance of the particle to itself \n",
    "        phi_log_prob = (self.K_XX-self.identity_mat2).matmul(self.score_func) / (self.num_particles-1)\n",
    "        phi_log_prob += (self.K_grad.sum(0) / (self.num_particles-1))\n",
    "        \n",
    "        return phi, phi_log_prob\n",
    "\n",
    "    def LD(self,X, with_noise=None):\n",
    "        \"\"\"\n",
    "        Compute the Langevin Dynamics Gradient given a set of particles\n",
    "        Inputs:\n",
    "            X: A tensor of particles\n",
    "        Outputs:\n",
    "            phi: The LD gradient\n",
    "        \"\"\"\n",
    "        # Make X requires grad\n",
    "        X = X.requires_grad_(True)\n",
    "        # Compute the gradient of the log probability over the particles\n",
    "        ld = autograd.grad(self.P.log_prob(X).sum(), X, retain_graph=True, create_graph=True)[0].reshape((self.num_particles, self.particles_dim))\n",
    "        # Add the noise to the gradient if with_noise is True\n",
    "        if with_noise:\n",
    "            ld = ld + 2 * np.sqrt(self.optim.lr) * (self.init_dist_ld.sample((self.num_particles,))).to(device) / self.optim.lr\n",
    "        return ld\n",
    "    \n",
    "\n",
    "    def update_X_list(self, X):\n",
    "        self.X_2 = X\n",
    "        \n",
    "    def update_V_list(self, V):\n",
    "        self.V_2 = V\n",
    "    def HMC(self, X, V=None):\n",
    "        \n",
    "        if (self.X_2 == None):\n",
    "            # store X_0 and V_0\n",
    "            self.X_2 = X\n",
    "            self.V_2 = V\n",
    "\n",
    "            # compute X_1\n",
    "            X = X.requires_grad_(True)\n",
    "            log_prob = self.P.log_prob(X).sum()\n",
    "            grad_log_prob = autograd.grad(log_prob.sum(), X, retain_graph=True, create_graph=True)[0].reshape((self.num_particles, self.particles_dim))\n",
    "            \n",
    "            return V + 0.5 * self.optim.lr *  grad_log_prob\n",
    "        \n",
    "        else:\n",
    "            self.update_X_list(X)\n",
    "\n",
    "            X_1 = X\n",
    "            X_2 = self.X_2\n",
    "            \n",
    "            # \\nabla_{x_{t-2}} logp(x_{t-2})\n",
    "            log_prob_t_2_2 = self.P.log_prob(X_2).sum()\n",
    "            grad_log_prob_t_2_2 = autograd.grad(log_prob_t_2_2.sum(), X_2, retain_graph=True, create_graph=True)[0].reshape((self.num_particles, self.particles_dim))\n",
    "\n",
    "            # \\nabla_{x_{t-1}} logp(x_{t-1})\n",
    "            log_prob_t_1_1 = self.P.log_prob(X_1).sum()\n",
    "            grad_log_prob_t_1_1 = autograd.grad(log_prob_t_1_1.sum(), X_1, retain_graph=True, create_graph=True)[0].reshape((self.num_particles, self.particles_dim))\n",
    "\n",
    "            # \\nabla_{x_{t-2}} logp(x_{t-1})\n",
    "            log_prob_t_2_1 = self.P.log_prob(X_1).sum()\n",
    "            grad_log_prob_t_2_1 = autograd.grad(log_prob_t_2_1.sum(), X_2, retain_graph=True, create_graph=True)[0].reshape((self.num_particles, self.particles_dim))\n",
    "            \n",
    "            X = self.V_2 + 0.5 * self.optim.lr * (grad_log_prob_t_2_2 + grad_log_prob_t_2_1 + grad_log_prob_t_1_1)\n",
    "            \n",
    "            V = self.V_2 + 0.5 * self.optim.lr * ( grad_log_prob_t_2_2 + grad_log_prob_t_2_1 )\n",
    "\n",
    "            \n",
    "            # print(self.V_2)\n",
    "            \n",
    "            self.update_V_list(V)\n",
    "            \n",
    "            return X\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def compute_logprob(self, phi, X):\n",
    "        \"\"\"\n",
    "        Compute the log probability of the given particles in 3 ways \n",
    "        Inputs:\n",
    "            X: A tensor of particles\n",
    "            phi: Update rule\n",
    "        Outputs:\n",
    "            self.logp_line1: Log probability using line 1 in the paper\n",
    "            self.logp_line2: Log probability using line 2 in the paper\n",
    "        \"\"\"\n",
    "        # Compute the derivative of the update rule over each particle separately\n",
    "        \n",
    "        grad_phi =[]\n",
    "        for i in range(len(X)):\n",
    "            grad_phi_tmp = []\n",
    "            for j in range(self.particles_dim):\n",
    "                grad_ = autograd.grad(phi[i][j], X, retain_graph=True)[0][i].detach()\n",
    "                grad_phi_tmp.append(grad_)\n",
    "            grad_phi.append(torch.stack(grad_phi_tmp))\n",
    "        self.grad_phi = torch.stack(grad_phi) \n",
    "        # Compute the log probability (Line 1)\n",
    "        self.logp_line1 = self.logp_line1 - torch.log(torch.abs(torch.det(self.identity_mat + self.optim.lr * self.grad_phi)))\n",
    "        # Compute the log probability (Line 2)\n",
    "        grad_phi_trace = torch.stack( [torch.trace(grad_phi[i]) for i in range(len(grad_phi))] ) \n",
    "        self.logp_line2 = self.logp_line2 - self.optim.lr * grad_phi_trace\n",
    "        # Compute the log probability (Line 2)\n",
    "        '''\n",
    "        line3_term1 = (self.K_grad * self.score_func.unsqueeze(0)).sum(-1).sum(1)/(self.num_particles-1)\n",
    "        line3_term2 = -2 * self.K_gamma * (( self.K_grad.permute(1,0,2) * self.K_diff).sum(-1) - self.particles_dim * (self.K_XX - self.identity_mat2) ).sum(0)/(self.num_particles-1)\n",
    "        self.logp_line3 = self.logp_line3 - self.optim.lr * (line3_term1 + line3_term2)\n",
    "        '''\n",
    "        \n",
    "    def step(self, X, V, alg):\n",
    "        \"\"\"\n",
    "        Perform one update step\n",
    "        Inputs:\n",
    "            X: A tensor of articles\n",
    "            alg: The name of the algorithm that should be used\n",
    "        Outputs:\n",
    "            X: Updated particles\n",
    "            phi_X: Update rule\n",
    "        \"\"\"\n",
    "        if alg == 'svgd':\n",
    "            # Compute the SVGD update\n",
    "            phi_X, phi_X_log_prob = self.SVGD(X) \n",
    "        elif alg == 'ld':\n",
    "            # Compute the Langevin Dynamics update\n",
    "            phi_X = self.LD(X, with_noise=True) \n",
    "            phi_X_log_prob = phi_X\n",
    "        elif alg == 'dld':\n",
    "            # Compute the Langevin Dynamics update\n",
    "            phi_X = self.LD(X, with_noise=False) \n",
    "            phi_X_log_prob = phi_X\n",
    "        elif alg == 'hmc':\n",
    "            # Compute the Hamiltionian Monte-carlo update\n",
    "            phi_X = self.HMC(X, V) \n",
    "            phi_X_log_prob = phi_X\n",
    "            \n",
    "        # Update the particles using the computed update \n",
    "        X_new = self.optim.step(X, phi_X) \n",
    "        self.phi_X = phi_X\n",
    "        # Compute the log probability for the updated particles\n",
    "        self.compute_logprob(phi_X_log_prob, X)\n",
    "        # Detach the particles to prepare for the next iteration\n",
    "        if alg in ['svgd', 'ld']:\n",
    "            X = X_new.detach() \n",
    "        else:\n",
    "            X = X_new\n",
    "\n",
    "        return X, phi_X \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SwVMAObJ2XFp"
   },
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dudEH5vAe4Fn"
   },
   "outputs": [],
   "source": [
    "\n",
    "def my_experiment(dim, n_gmm, num_particles, num_steps, kernel_sigma, gmm_std, lr, mu_init, sigma_init, sigma_V_init, dist_type, run_alg):\n",
    "    \"\"\"\n",
    "    Perform one experiment of running SVGD and computing the entropy\n",
    "    Inputs:\n",
    "        dim: Number of dimensions of the particles\n",
    "        n_gmm: Number of mixutres of the target GMM distribution\n",
    "        num_particles: Number of particles\n",
    "        num_step: Number of update steps\n",
    "        kernel_sigma: Kernel standard diviation\n",
    "        gmm_std: The target distribution standard diviation\n",
    "        lr: Learning rate\n",
    "        sigma_init: Standard deviation of the initial distribution\n",
    "        mu_init: Mean of the initial distribution\n",
    "        dist: Type of the target distribution\n",
    "    Outputs:\n",
    "        sampler_entr_svgd: The SVGD Entropy\n",
    "        sampler_entr_ld: The LD Entropy\n",
    "        gt_entr: The ground truth Entropy\n",
    "        charts: The list of density plots with particles.(a plot for each svgd step)\n",
    "    \"\"\"\n",
    "\n",
    "    # Creating the initial distribution\n",
    "    mu = torch.full((dim,), mu_init).to(device).to(torch.float32)\n",
    "    sigma = (torch.eye(dim) * sigma_init).to(device)\n",
    "    init_dist = torch.distributions.MultivariateNormal(mu,covariance_matrix=sigma)\n",
    "\n",
    "    # Sampling a tensor of particles from the initial distribution\n",
    "    X_init = init_dist.sample((num_particles,)).to(device)\n",
    "\n",
    "    init_V_dist = torch.distributions.MultivariateNormal(torch.zeros(dim).to(device),covariance_matrix = (torch.eye(dim) * sigma_V_init).to(device))\n",
    "    V_init = init_V_dist.sample((num_particles,)).to(device)\n",
    "    \n",
    "    if dist_type == 'gmm': \n",
    "        # Figures 4b, high dimensionality figures\n",
    "        dist = GMMDist(dim=dim, n_gmm=n_gmm, sigma=gmm_std)\n",
    "    elif dist_type == 'gauss':\n",
    "        # Figure 4a\n",
    "        dist = torch.distributions.MultivariateNormal(torch.Tensor([-0.6871,0.8010]).to(device),covariance_matrix= 5 *torch.Tensor([[0.2260,0.1652],[0.1652,0.6779]]).to(device)) \n",
    "\n",
    "    # Create an entropy experiment object \n",
    "    experiment = Entropy(dist, RBF(kernel_sigma, num_particles), Optim(lr), num_particles=num_particles, particles_dim=dim) \n",
    "\n",
    "    # Generate plots only if dim == 2\n",
    "    if dim == 2:\n",
    "        gauss_chart = get_density_chart(dist, d=7.0, step=0.1, dist_type=dist_type) \n",
    "        init_chart = gauss_chart + get_particles_chart(X_init.cpu().numpy())\n",
    "    else:\n",
    "        init_chart = None\n",
    "    \n",
    "    # Compute the groud truth entropy\n",
    "    gt_entr = - dist.log_prob(dist.sample((500,))).mean().cpu().item()\n",
    "\n",
    "    def main_loop(alg, X, V, steps):\n",
    "        \"\"\"\n",
    "        Perform SVGD updates\n",
    "        Inputs:\n",
    "            X: A tensor of particles\n",
    "            steps: The number of SVGD steps\n",
    "        Outputs:\n",
    "            sampler_entr_svgd: The SVGD Entropy\n",
    "            gt_entr: The ground truth Entropy\n",
    "            charts: The list of density plots with particles.(a plot for each svgd step)\n",
    "        \"\"\"\n",
    "        charts = []\n",
    "        X_svgd_=[]\n",
    "        for t in range(steps): \n",
    "            # Perform one update step\n",
    "            X, phi_X = experiment.step(X, V, alg)\n",
    "            # Save the intermidiate position of the particles\n",
    "            X_svgd_.append(X.clone())\n",
    "            # Generate plots only if dim == 2\n",
    "            if dim == 2:\n",
    "                chart_ = gauss_chart + get_particles_chart(X.detach().cpu().numpy())\n",
    "                charts.append(chart_)\n",
    "\n",
    "        X_svgd_ = torch.stack(X_svgd_)\n",
    "        \n",
    "        # Compute the entropy of the particles using the log probability\n",
    "        sampler_entr =  -(init_dist.log_prob(X_init) + experiment.logp_line2).mean().item()\n",
    "        \n",
    "        print('################# sampler_entr: ', alg, sampler_entr)\n",
    "        # print('################# diff X_t X_t-1: ', torch.norm(experiment.phi_X).detach().cpu().numpy())\n",
    "\n",
    "        # Print line 1 and line 2\n",
    "        # print(t, ' entropy svgd (line 1): ',  -(init_dist.log_prob(X_init) + experiment.logp_line1).mean().item())\n",
    "        # print(t, ' entropy svgd (line 2): ',  -(init_dist.log_prob(X_init) + experiment.logp_line2).mean().item())\n",
    "        \n",
    "        return sampler_entr, charts\n",
    "    \n",
    "    # Print the ground truth entropy\n",
    "    # print('entropy gt (logp): ', gt_entr)  \n",
    "    # print()\n",
    "\n",
    "    # Run SVGD\n",
    "    if run_alg['svgd']:\n",
    "        # print('___________SVGD___________')\n",
    "        sampler_entr_svgd, charts_svgd = main_loop('svgd', X_init.clone().detach(), None, steps=num_steps[0])\n",
    "    else:\n",
    "        sampler_entr_svgd, charts_svgd = None, None\n",
    "    \n",
    "    # Run Lengevin Dynamics\n",
    "    if run_alg['ld']:\n",
    "        # print('___________LD___________')\n",
    "        sampler_entr_ld, charts_ld = main_loop('ld', X_init.clone().detach(), None, steps=num_steps[1])\n",
    "    else:\n",
    "        sampler_entr_ld, charts_ld = None, None\n",
    "    \n",
    "    # Run Deterministic Lengevin Dynamics\n",
    "    if run_alg['dld']:\n",
    "        # print('___________DLD___________')\n",
    "        sampler_entr_dld, charts_dld = main_loop('dld', X_init.clone().detach(), None, steps=num_steps[1])\n",
    "    else:\n",
    "        sampler_entr_dld, charts_dld = None, None\n",
    "    \n",
    "    # Run HMC\n",
    "    if run_alg['hmc']:\n",
    "        # print('___________HMC___________')\n",
    "        sampler_entr_hmc, charts_hmc = main_loop('hmc', X_init.clone().detach(), V_init.clone(), steps=num_steps[2])\n",
    "    else:\n",
    "        sampler_entr_hmc, charts_hmc = None, None\n",
    "    \n",
    "    \n",
    "    return init_chart, sampler_entr_svgd, sampler_entr_ld, sampler_entr_hmc, sampler_entr_dld , gt_entr, charts_svgd, charts_ld, charts_hmc, charts_dld\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce Paper Figures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather to use the GMM class or not (Figure 4a: gmm == False)\n",
    "dist_type = 'gauss'\n",
    "# Dimension of the particles\n",
    "dim = 2\n",
    "# Learning Rate\n",
    "lr = 0.5\n",
    "# Number of mixtures\n",
    "n_gmm = 1\n",
    "# Target distribution's standard deviation, means are computed automatically\n",
    "gmm_std = 1\n",
    "# Number of particles\n",
    "num_particles = 200\n",
    "# Number of update steps\n",
    "num_steps = [100, 20, 15]\n",
    "# Kernel variance\n",
    "kernel_variance = 'forth'\n",
    "# Standard deviation of the initial distribution HMC\n",
    "sigma_V_init = 5\n",
    "# Dictionary used to select whare algorithms to run\n",
    "run_alg = {'svgd':True, 'ld':True, 'dld':True, 'hmc':True}\n",
    "\n",
    "\n",
    "\n",
    "# Mean of the initial distribution\n",
    "mu_init = 0\n",
    "# Standard deviation of the initial distribution\n",
    "sigma_init = 6\n",
    "# Do one experiment for initial distribution N(0, 6)\n",
    "init_chart, sampler_entr_svgd, sampler_entr_ld, sampler_entr_hmc, sampler_entr_dld, gt_entr, charts_svgd, charts_ld, charts_hmc, charts_dld = my_experiment(dim, n_gmm, num_particles, num_steps, kernel_variance, gmm_std, lr, mu_init, sigma_init, sigma_V_init, dist_type, run_alg)\n",
    "# print('##############################')\n",
    "\n",
    "lr = 0.5\n",
    "# Number of update steps\n",
    "num_steps = [200, 20, 50]\n",
    "# Mean of the initial distribution\n",
    "mu_init = 4\n",
    "# Standard deviation of the initial distribution\n",
    "sigma_init = 0.2\n",
    "# Do one experiment for initial distribution N(4, 0.2)\n",
    "init_chart2, sampler_entr_svgd2, sampler_entr_ld2, sampler_entr_hmc2, sampler_entr_dld2, gt_entr2, charts_svgd2, charts_ld2, charts_hmc2, charts_dld2 = my_experiment(dim, n_gmm, num_particles, num_steps, kernel_variance, gmm_std, lr, mu_init, sigma_init, sigma_V_init, dist_type, run_alg)\n",
    "\n",
    "# Concatenate multiple plots together\n",
    "alt.vconcat(init_chart | charts_svgd[-1] | charts_dld[-1] | charts_ld[-1] | charts_hmc[-1], init_chart2 | charts_svgd2[-1] | charts_dld2[-1] | charts_ld2[-1] | charts_hmc2[-1])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8360f93b682846d9db2e24bd21c06b219fd218969b51c91bbf4fd90fc42d351f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
