# -*- coding: utf-8 -*-
"""Copy of Entropy_toy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bqVXC6JgiFEicRptOpTy-kB8G9Mr6tNX

# Imports
"""
import math
import torch
import numpy as np
from torch import autograd
import altair as alt
import matplotlib.pyplot as plt
import pandas as pd
from tqdm import tqdm
# from altair_saver import save as alt_save
alt.data_transformers.enable('default', max_rows=None)

"""# Global Variables"""

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
const = 1

"""# Plotting Functions"""

def get_density_chart(P, d=7.0, step=0.1):
    xv, yv = torch.meshgrid([torch.arange(-d, d, step), torch.arange(-d, d, step)])
    pos_xy = torch.cat((xv.unsqueeze(-1), yv.unsqueeze(-1)), dim=-1)
    p_xy = P.log_prob(pos_xy.to(device)).exp().unsqueeze(-1).cpu()

    df = torch.cat([pos_xy, p_xy], dim=-1).numpy()
    df = pd.DataFrame({
        'x': df[:, :, 0].ravel(),
        'y': df[:, :, 1].ravel(),
        'p': df[:, :, 2].ravel(),})

    chart = alt.Chart(df).mark_point().encode(
    x='x:Q',
    y='y:Q',
    color=alt.Color('p:Q', scale=alt.Scale(scheme='viridis')),
    tooltip=['x','y','p'])

    return chart


def get_particles_chart(X, X_svgd=None):
    df = pd.DataFrame({
        'x': X[:, 0],
        'y': X[:, 1],})

    chart = alt.Chart(df).mark_circle(color='red').encode(x='x:Q',y='y:Q')

    if X_svgd is not None:
        #import pdb; pdb.set_trace()
        for i in range(np.shape(X_svgd)[1]):
            df_trajectory = pd.DataFrame({'x': X_svgd[:,i,0],'y': X_svgd[:,i,1],})
            chart += alt.Chart(df_trajectory).mark_line().mark_circle(color='green').encode(x='x:Q',y='y:Q')

    return chart

"""# Kernels"""

# RBF Kernel 
class RBF:
    def __init__(self, sigma=None):
        #self.sigma = sigma
        self.sigma = 10.0 #sigma

    def forward(self, input_1, input_2):
        _, out_dim1 = input_1.size()[-2:]
        _, out_dim2 = input_2.size()[-2:]
        num_particles = input_2.size()[-2]
        assert out_dim1 == out_dim2
        
        # Compute the pairwise distances of left and right particles.
        diff = input_1.unsqueeze(-2) - input_2.unsqueeze(-3)
        dist_sq = diff.pow(2).sum(-1)
        dist_sq = dist_sq.unsqueeze(-1)
        
        h = None
        # Get median.
        if self.sigma is None:
            median_sq = torch.median(dist_sq.detach().reshape(-1, num_particles*num_particles), dim=1)[0]
            median_sq = median_sq.unsqueeze(1).unsqueeze(1)
            h = median_sq / (2 * np.log(num_particles + 1.))
            sigma = const * torch.sqrt(h)
        else:
            sigma = self.sigma

        gamma = 1.0 / (1e-8 + 2 * sigma**2) 
        #gamma = gamma * 0.1
        
        kappa = (-gamma * dist_sq).exp() 
        
        kappa_grad = -2. * (diff * gamma) * kappa
        return kappa.squeeze(), diff, h, kappa_grad, gamma


class GMMDist(object):
    def __init__(self, dim, n_gmm):
        def _compute_mu(i):
            return 4.0 * torch.Tensor([[torch.tensor(i * math.pi / (n_gmm//2)).sin(),torch.tensor(i * math.pi / (n_gmm//2)).cos()]])
        
        self.mix_probs = 0.25 * torch.ones(n_gmm).to(device)
        # self.means = torch.stack([5 * torch.ones(dim), -torch.ones(dim) * 5], dim=0)
        # self.mix_probs = torch.tensor([0.1, 0.1, 0.8])
        # self.means = torch.stack([5 * torch.ones(dim), torch.zeros(dim), -torch.ones(dim) * 5], dim=0)
        self.means = torch.cat([_compute_mu(i) for i in range(n_gmm)], dim=0).to(device)
        #self.means = torch.stack([5 * torch.ones(dim).to(device), -torch.ones(dim).to(device) * 5], dim=0)
        self.sigma = 1.0
        self.std = torch.stack([torch.ones(dim).to(device) * self.sigma for i in range(len(self.mix_probs))], dim=0)

    def sample(self, n):
        n = n[0]
        mix_idx = torch.multinomial(self.mix_probs, n, replacement=True)
        means = self.means[mix_idx]
        stds = self.std[mix_idx]
        return torch.randn_like(means) * stds + means

    def log_prob(self, samples):
        logps = []
        for i in range(len(self.mix_probs)):
            logps.append((-((samples - self.means[i]) ** 2).sum(dim=-1) / (2 * self.sigma ** 2) - 0.5 * np.log(
                2 * np.pi * self.sigma ** 2)) + self.mix_probs[i].log())
        logp = torch.logsumexp(torch.stack(logps, dim=0), dim=0)
        return logp


"""# Optimizer"""
class Optim():
    def __init__(self, lr=None):
        self.v_dx = 0
        self.lr = lr
        ####
        self.beta_2 = 0.999
        
    
    def step(self,x, dx, dq, adaptive_lr=None): 
        dx = dx.view(x.size())
        if not adaptive_lr:
            self.lr_coeff = self.lr
        ###
        else:
            self.v_dx = self.beta_2 * self.v_dx + (1-self.beta_2) * dq**2
            v_x_hat = self.v_dx/(1-self.beta_2)

            self.lr_coeff = self.lr * 1/(torch.sqrt(v_x_hat)+1e-8)
            self.lr_coeff = self.lr_coeff.mean()
        print("######################## ", self.lr_coeff)
        
        ### 
        x = x + self.lr_coeff * dx 
        return x

"""# Entropy Toy Class"""

class Entropy_toy():
    def __init__(self, P, K, optimizer, num_particles, particles_dim, with_logprob):
        self.P = P
        self.optim = optimizer
        self.num_particles = num_particles
        self.particles_dim = particles_dim
        self.with_logprob = with_logprob
        self.K = K

        # svgd 
        mu_ld_noise = torch.zeros((self.particles_dim,)) 
        sigma_ld_noise = torch.eye(self.particles_dim) * 0.05
        self.init_dist_ld = torch.distributions.MultivariateNormal(mu_ld_noise,covariance_matrix=sigma_ld_noise)
        self.identity_mat = torch.eye(self.particles_dim).to(device)

        # entropy
        self.logp_line1 = 0
        self.logp_line2 = 0
    
    def SVGD(self,X):
        # Stein Variational Gradient Descent
        X = X.requires_grad_(True)
        log_prob = self.P.log_prob(X)
        score_func = autograd.grad(log_prob.sum(), X)[0].reshape(X.size())
        print('************ score_func ', (score_func**2).sum(-1).mean() )

        self.score_func = score_func.reshape(X.size())
        self.K_XX, self.K_diff, self.K_h, self.K_grad, self.K_gamma = self.K.forward(X, X)        
        self.num_particles =  X.size(0)
        self.phi_term1 = self.K_XX.matmul(score_func) / X.size(0)
        self.phi_term2 = self.K_grad.sum(0) / X.size(0)
        phi = self.phi_term1 + self.phi_term2

        phi_entropy = (self.K_XX-torch.eye(X.size(0)).to(device)).matmul(score_func) / X.size(0) + self.phi_term2
        
        phi_entropy = phi_entropy * (torch.norm(phi_entropy, dim=1).view(-1,1) > 0.1).int() 

        #print('kernel: ',self.K_XX[0])
        
        return phi, phi_entropy

    def LD(self,X, with_noise=None):
        # Langevin Dynamics
        X = X.requires_grad_(True)
        log_prob = self.P.log_prob(X).sum()
        ld = autograd.grad(log_prob.sum(), X, retain_graph=True, create_graph=True)[0].reshape((self.num_particles, self.particles_dim))
        if with_noise:
            ld = ld + 2* (np.sqrt(self.optim.lr) * self.init_dist_ld.sample((self.num_particles,))).to(device) / self.optim.lr
        return ld

    def compute_entropy(self, phi, X):
        grad_phi =[]
        for i in range(len(X)):
            grad_phi_tmp = []
            for j in range(X.size(1)):
                grad_ = autograd.grad(phi[i][j], X, retain_graph=True)[0][i].detach()
                grad_phi_tmp.append(grad_)
            grad_phi.append(torch.stack(grad_phi_tmp))

        self.grad_phi = torch.stack(grad_phi) 
        self.logp_line1 = self.logp_line1 - torch.log(torch.abs(torch.det(self.identity_mat + self.optim.lr_coeff * self.grad_phi)))
        
        grad_phi_trace = torch.stack( [torch.trace(grad_phi[i]) for i in range(len(grad_phi))] ) 
        self.logp_line2 = self.logp_line2 - self.optim.lr_coeff * grad_phi_trace


    def step(self, X, itr, alg=None, adaptive_lr=None):
        if alg == 'svgd':
            phi_X, phi_X_entropy = self.SVGD(X) 
        elif alg == 'ld':
            phi_X = self.LD(X, with_noise=False) 
            phi_X_entropy = phi_X
        
        # print('Phi :', phi_X[0])
        X_new = self.optim.step(X, phi_X, self.score_func.mean(0), adaptive_lr=adaptive_lr) 

        print('############## PHIIiiii', phi_X.mean())

        if self.with_logprob: 
            self.compute_entropy(phi_X_entropy, X)
        
        X = X_new.detach() 
        
        return X, phi_X

"""# Initializations"""

lr = .2
dim = 2
# number of particles
n = 50

gmm = 1
sig = 1
# Initial distribution of SVGD
if (gmm == 1):
    mu = torch.zeros((dim,)) + 400
    sigma_ = 0.2 #6
else:
    mu = torch.zeros((dim,)) 
    #mu[0]=-4
    #mu[1]=4
    sigma_ = 0.2 #6
#
sigma = torch.eye(dim) * sigma_
init_dist = torch.distributions.MultivariateNormal(mu.to(device),covariance_matrix=sigma.to(device))


X_init = init_dist.sample((n,))

# Target distribution of SVGD
if (gmm == 1):
    #gauss = torch.distributions.MultivariateNormal(torch.Tensor([-0.6871,0.8010]).to(device),covariance_matrix=5 * torch.Tensor([[0.2260,0.1652],[0.1652,0.6779]]).to(device)
    sig = 0.2 #0.2
    print('sig ', sig)
    gauss = torch.distributions.MultivariateNormal(torch.Tensor([0.0,0.0]).to(device),covariance_matrix= sig * torch.Tensor([[1.0,0.0],[0.0,1.0]]).to(device))
else:
    gauss = GMMDist(dim=2, n_gmm=gmm)
    #gauss = MoG2(device=device)
# Initialize the experiment
experiment = Entropy_toy(gauss, RBF(), Optim(lr), num_particles=n, particles_dim=dim, with_logprob=True) 

# Plotting the Target distribution with the initial particles 
gauss_chart = get_density_chart(gauss, d=7.0, step=0.1) 
chart = gauss_chart + get_particles_chart(X_init.cpu().numpy())

# alt_save(chart, "./ToyExperiments/figs/init_gmm_" + str(gmm) + ".png")          
chart.save("./ToyExperiments/figs/init_gmm_" + str(gmm) + ".png")


# """# Main Loop"""

# # Main_loop
# charts = []
# term1 = []
# term2 = []

# def main_loop(alg, X, steps, adaptive_lr):

#     print('steps ', steps)

#     line_1 = []
#     line_2 = []

#     for t in range(steps):
#         print(t)
#         X, _ = experiment.step(X, t, alg, adaptive_lr)
#         #X_svgd_.append(X.clone())
        
#         #if (t%10)==0: 
#         #chart = gauss_chart + get_particles_chart(X.detach().cpu().numpy())
#         #chart_ = gauss_chart + get_particles_chart(X.detach().cpu().numpy(), torch.stack(X_svgd_).detach().cpu().numpy())
#         # term1.append(experiment.phi_term1)
#         # term2.append(experiment.phi_term2)
#         line_1.append( -(init_dist.log_prob(X_init) + experiment.logp_line1).mean().item() )
#         line_2.append( -(init_dist.log_prob(X_init) + experiment.logp_line2).mean().item() )
#         print(t, ' entropy svgd (line 1): ',  line_1[-1])
#         print(t, ' entropy svgd (line 2): ',  line_2[-1])
#         #print('sampler logprob ', experiment.logp_line1.mean()) 
#         #X_svgd_ = torch.stack(X_svgd_)
#         # Plotting the results 
        
#         if t == 10 : 
#             experiment.optim.lr_coeff = 0.08
        
#         if (t%100)==0: 
#             chart = gauss_chart + get_particles_chart(X.detach().cpu().numpy())
#             alt_save(chart, "./ToyExperiments/figs/gmm_"+str(gmm)+"_"+str(t)+'_'+str(sig)+".png")  
#             charts.append(chart)
#         #chart_ = gauss_chart + get_particles_chart(X.detach().cpu().numpy(), X_svgd_.detach().cpu().numpy())
#         print()
#         # print('entropy gt: ', gauss.entropy().item())  
#         print('entropy gt (logp): ', - gauss.log_prob(X).mean())  
#         print('entropy svgd/LD (line 1): ',  -(init_dist.log_prob(X_init) + experiment.logp_line1).mean().item())
#         print()
#         print('init_dist_entr_GT ', init_dist.log_prob(X_init).mean()) 
#         print('sampler logprob ', experiment.logp_line1.mean()) 
#     return charts, line_1, line_2


# """# Run SVGD"""
# charts, line_1, line_2 = main_loop('svgd', X_init.clone(), steps=500, adaptive_lr=True)
# alt_save(charts[-1], "./ToyExperiments/figs/gmm_"+str(gmm)+".png")  

# print('gmm: ', gmm)
# print('sig ', sig)
# """# Run Lengevin Dynamics"""
# #charts = main_loop('ld', X_init.clone(), steps=200)
# #charts[-1]
# plt.plot(np.arange(len(line_1)),line_1, c="r", label="line_1")
# plt.plot(np.arange(len(line_2)),line_2, c="b", label="line_2")
# plt.title('Comparison between the entropy of Line 1 and Line 2')
# plt.xlabel('Training iterations')
# plt.ylabel('Entropy')
# plt.legend()
# plt.savefig('./ToyExperiments/figs/line_1_line_2.png')
# plt.close()


